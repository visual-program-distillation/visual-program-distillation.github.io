<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models">
  <meta name="keywords" content="Vision Language Model, Instruction Tuning, LLM, VLM, Agents">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models</title>

<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-TX425GL8');</script>
<!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/tifa.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TX425GL8" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://yushi-hu.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://finegrainedrlhf.github.io/">
            Fine-Grained RLHF (NeurIPS 2023)
          </a>
          <a class="navbar-item" href="https://tifa-benchmark.github.io/">
            TIFA (ICCV 2023)
          </a>
          <a class="navbar-item" href="https://yushi-hu.github.io/promptcap_demo/">
            PromptCap (ICCV 2023)
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span style="color: rgb(178, 51, 51);"><b>Visual Program Distillation:</b></span> <br> Distilling Tools and Programmatic Reasoning into Vision-Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://yushi-hu.github.io/">Yushi Hu</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://otiliastr.github.io/">Otilia Stretcu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www2.cs.uic.edu/~clu/">Chun-Ta Lu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://visual-program-distillation.github.io/">Krishnamurthy Viswanathan</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://visual-program-distillation.github.io/">Enming Luo</a><sup>1</sup>,</span>
              <br>
            <span class="author-block">
                    <a href="http://www.ranjaykrishna.com/">Ranjay Krishna</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://visual-program-distillation.github.io/">Ariel Fuxman</a><sup>1</sup></span>

          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Google Research,  </span>
            <span class="author-block"><sup>2</sup>University of Washington</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2303.11897"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2211.09699.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              
              <!-- Code Link. -->

            </div>

                          

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

            <h2 class="subtitle has-text-centered">
              We introduce Visual Program Distillation (VPD), a framework that leverages LLM-generated programs and vison tools to
              synthesis multimodal chain-of-thought training data.
            </h2>
      
      <img src="./static/images/website teaser.jpg" alt="VPD teaser">
      <h2 class="subtitle has-text-centered">
              VPD distills the reasoning ability of LLMs and the perception ability of vision tools into a single VLM.
              VLMs instruction-tuned with VPD are able to solve complex visual tasks with a single forward pass, while producing human-interpretable and faithful reasoning steps. 
            </h2>

      <img src="./static/images/VPD website fig1.jpg" alt="VPD radar">

      <h2 class="subtitle has-text-centered">
                    Our generalist models trained with
                    VPD, PaLI-3-VPD (5B) and PaLI-X-VPD (55B), outperform prior VLMs on a broad range of tasks and set new SOTAs.
                  </h2>
      
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Solving complex visual tasks such as "Who invented the musical instrument on
            the right?" involves a composition of skills: understanding space, recognizing
            instruments, and also retrieving prior knowledge. Recent work shows promise by
            decomposing such tasks using a large language model (LLM) into an executable
            program that invokes specialized vision models. However, generated programs are
            error-prone: they omit necessary steps, include spurious ones, and are unable
            to recover when the specialized models give incorrect outputs. Moreover, they
            require loading multiple models, incurring high latency and computation costs.
          </p>

          <p>
            We propose <b>Visual Program Distillation (VPD)</b>, an instruction tuning framework
            that produces a vision-language model (VLM) capable of solving complex visual
            tasks with a single forward pass. VPD distills the reasoning ability of LLMs by
            using them to sample multiple candidate programs, which are then executed and
            verified to identify a correct one. It translates each correct program into a
            language description of the reasoning steps, which are then distilled into a
            VLM.
          </p>

          <p>
            Extensive experiments show that VPD improves the VLM's ability to count,
            understand spatial relations, and reason compositionally. Our VPD-trained
            PaLI-X outperforms all prior VLMs, achieving state-of-the-art performance
            across complex vision tasks, including MMBench, OK-VQA, A-OKVQA, TallyQA, POPE,
            and Hateful Memes. An evaluation with human annotators also confirms that VPD
            improves model response factuality and consistency. Finally, experiments on
            content moderation demonstrate that VPD is also helpful for adaptation to
            real-world applications with limited data.
          </p>

        </div>
      </div>
    </div>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Approach</h2>
      <img src="./static/images/VPD website fig2.jpg" alt="VPD approach">
      <div class="content has-text-justified">
        <p>
          The figure above shows the overall framework of VPD.
          VPD contains two stages: <b>Program generation and verification</b> and <b>Distilling step-by-step</b>. 
          Program generation and verification contains following steps</b>:<br>
          1. Program generation with LLM <br>
          2. Program execution with vision modules <br>
          3. Program filtering <br>
          4. Converting program execution trace into chains-of-thoughts <br>
          Given the synthesized CoT data, we fine-tune VLMs to output these chains-of-thoughts, using the same approach as in Distilling step-by-step. <br>
        </p>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Comparison between LLaVA and VPD instruction tuning</h2>
      <img src="static/images/VPD website fig3.jpg">
      <div class="content has-text-justified">
        <p>
          So, how does VPD differ from prior approaches for generating visual instruction tuning data?
          Here we show examples of how LLaVA and VPD generate data.
          LLaVA prompts LLM with image captions and let LLM generate task inputs and outputs.
          However, <span style="color: red;"><b>image captions are coarse representation of images, and does not contain fine-grained attributes and relations.</b></span>
          Bounding boxes may complement that, but LLMs are not good at reading bounding boxes, and only densely labeled datasets like COCO can be used.
          Also, <span style="color: red;"><b>LLM generations suffers from hallucination and spurious reasoning steps.</b></span>
          Even GPT-4V is still not reliable enough to generate faithful reasoning steps for complex tasks.
          In contrast, VPD generates data by sampling programs from LLMs, and then use existing vision tools to get reasoning steps.
          <span style="color: red;"><b>VPD works on any images, have fine-grained visual details, and contains more factual and consistent reasoning steps.</b></span>
        </p>

      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Quantitative results</h2>
      <img src="static/images/VPD website fig5.png">
      <div class="content has-text-justified">
        <p>
          In generalist setting, our PaLI-X-VPD sets the new state-of-the-art on all benchmarks. PaLI-3-VPD outperforms
          prior 13B+ VLMs on most benchmarks. Also, VPD variants outperform instruction-tuning baselines.
          VPD is also helpful for adaptation to real-word tasks with limited data.
          We experiment with VPD on Hateful Memes dataset. We sets the new SOTA for both supervised and
          unsupervised settings. Surprisingly, unsupervised PaLI-X-VPD even outperforms strong supervised baselines
          trained with 8,500 labels.
        </p>

      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Human evaluation: VPD makes models more factual and consistent</h2>
      <img src="static/images/VPD website fig6.png">
      <div class="content has-text-justified">
        <p>
          Human evaluation shows that the long-form outputs of VPD models are more accurent, consistent, and faithful compared to the baselines instruction-tuned with LLM generations.
        </p>

      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Demos on content moderation</h2>
      <img src="static/images/VPD website fig7.jpg">
      <div class="content has-text-justified">
        <p>
          Here we show some demos of applying VPD on content moderation task Hateful Memes. 
          There are two settings. For the unsupervised settings, we do not use any human labels.
          For the supervised settings, we use 8,500 "yes" and "no" labels to fine-tune the model.
          Our models are able to generate human-interpretable reasoning steps, and are able to detect hateful memes.
          The unsupervised model works surprisingly well. We also show a failure case of the unsuperivsed model.
        </p>

      </div>
    </div>
  </div>
</section>





<!-- <section class="hero is-small"> -->
  <!-- <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Human evaluation results</h2>
      <img src="static/images/VPD website fig5.png">
      <div class="content has-text-justified">
        <p>
          In generalist setting, our PaLI-X-VPD sets the new state-of-the-art on all benchmarks. PaLI-3-VPD outperforms
          prior 13B+ VLMs on most benchmarks. Also, VPD variants outperform instruction-tuning baselines.
          We also see similar results on Hateful memes, in which we sets the new SOTA for both supervised and
          unsupervised settings. Surprisingly, unsupervised PaLI-X-VPD even outperforms strong supervised baselines
          trained with 8,500 labels.
        </p>

      </div>
    </div>
  </div>
</section> -->


<section class="section is-light" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{hu2023tifa,
title={TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering},
author={Hu, Yushi and Liu, Benlin and Kasai, Jungo and Wang, Yizhong and Ostendorf, Mari and Krishna, Ranjay and Smith,
Noah A},
journal={arXiv preprint arXiv:2303.11897},
year={2023}
}
</code></pre>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
